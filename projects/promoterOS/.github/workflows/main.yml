name: PromoterOS CI/CD Pipeline

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]
  workflow_dispatch:
    inputs:
      environment:
        description: 'Environment to deploy to'
        required: true
        default: 'staging'
        type: choice
        options:
        - dev
        - staging
        - prod

env:
  AWS_REGION: us-east-1
  AWS_ACCOUNT_ID: 681214184463
  ECR_REGISTRY: 681214184463.dkr.ecr.us-east-1.amazonaws.com
  EKS_CLUSTER: promoteros-eks
  DOCKER_BUILDKIT: 1

jobs:
  security-scan:
    name: Security Scanning
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      
      - name: Run Trivy security scan
        uses: aquasecurity/trivy-action@master
        with:
          scan-type: 'fs'
          scan-ref: '.'
          format: 'sarif'
          output: 'trivy-results.sarif'
          severity: 'CRITICAL,HIGH'
          exit-code: '1'
      
      - name: Upload Trivy results to GitHub Security
        uses: github/codeql-action/upload-sarif@v3
        if: always()
        with:
          sarif_file: 'trivy-results.sarif'
      
      - name: Run Snyk security scan
        uses: snyk/actions/node@master
        continue-on-error: true
        env:
          SNYK_TOKEN: ${{ secrets.SNYK_TOKEN }}
        with:
          args: --severity-threshold=high
      
      - name: Run Semgrep SAST
        uses: returntocorp/semgrep-action@v1
        with:
          config: auto
          generateSarif: true
      
      - name: Check for secrets with Trufflehog
        uses: trufflesecurity/trufflehog@main
        with:
          path: ./
          base: ${{ github.event.repository.default_branch }}
          head: HEAD

  test:
    name: Test Suite
    runs-on: ubuntu-latest
    needs: security-scan
    services:
      postgres:
        image: pgvector/pgvector:pg15
        env:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: promoteros_test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
      
      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'
          cache: 'npm'
      
      - name: Setup Go
        uses: actions/setup-go@v5
        with:
          go-version: '1.21'
      
      - name: Install dependencies
        run: |
          npm ci
          cd services/api-gateway && go mod download
          cd ../scraper-suite && npm ci
      
      - name: Run database migrations
        env:
          DATABASE_URL: postgresql://postgres:postgres@localhost:5432/promoteros_test
        run: |
          npm install -g migrate
          migrate -path db/migrations -database "$DATABASE_URL" up
      
      - name: Run unit tests
        run: |
          npm run test:unit -- --coverage
          cd services/api-gateway && go test -v -race -coverprofile=coverage.out ./...
      
      - name: Run integration tests
        env:
          DATABASE_URL: postgresql://postgres:postgres@localhost:5432/promoteros_test
          REDIS_URL: redis://localhost:6379
        run: npm run test:integration
      
      - name: Run E2E tests
        run: npm run test:e2e
      
      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v3
        with:
          files: ./coverage/lcov.info,./services/api-gateway/coverage.out
          flags: unittests
          name: codecov-umbrella
          fail_ci_if_error: true
      
      - name: Check test coverage
        run: |
          COVERAGE=$(jq '.total.lines.pct' coverage/coverage-summary.json)
          echo "Code coverage: ${COVERAGE}%"
          if (( $(echo "$COVERAGE < 80" | bc -l) )); then
            echo "Coverage is below 80% threshold"
            exit 1
          fi

  build:
    name: Build and Push Images
    runs-on: ubuntu-latest
    needs: test
    if: github.event_name == 'push' || github.event_name == 'workflow_dispatch'
    strategy:
      matrix:
        service:
          - api-gateway
          - scraper-suite
          - ml-inference
          - realtime-ws
          - worker-queue
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
      
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2
      
      - name: Build and push Docker image
        uses: docker/build-push-action@v5
        with:
          context: ./services/${{ matrix.service }}
          file: ./services/${{ matrix.service }}/Dockerfile
          push: true
          tags: |
            ${{ env.ECR_REGISTRY }}/promoteros-${{ matrix.service }}:${{ github.sha }}
            ${{ env.ECR_REGISTRY }}/promoteros-${{ matrix.service }}:latest
          cache-from: type=gha
          cache-to: type=gha,mode=max
          build-args: |
            VERSION=${{ github.sha }}
            BUILD_DATE=$(date -u +'%Y-%m-%dT%H:%M:%SZ')
      
      - name: Scan image with Trivy
        uses: aquasecurity/trivy-action@master
        with:
          image-ref: ${{ env.ECR_REGISTRY }}/promoteros-${{ matrix.service }}:${{ github.sha }}
          format: 'sarif'
          output: 'trivy-image-results.sarif'
      
      - name: Upload image scan results
        uses: github/codeql-action/upload-sarif@v3
        if: always()
        with:
          sarif_file: 'trivy-image-results.sarif'

  deploy-staging:
    name: Deploy to Staging
    runs-on: ubuntu-latest
    needs: build
    if: github.ref == 'refs/heads/develop'
    environment:
      name: staging
      url: https://staging.promoteros.candlefish.ai
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: Update kubeconfig
        run: |
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.EKS_CLUSTER }}
      
      - name: Install Helm
        uses: azure/setup-helm@v3
        with:
          version: '3.13.0'
      
      - name: Deploy with Helm
        run: |
          helm upgrade --install promoteros-staging ./helm/charts/promoteros \
            --namespace promoteros-staging \
            --create-namespace \
            --values ./helm/values/staging.yaml \
            --set image.tag=${{ github.sha }} \
            --wait \
            --timeout 10m
      
      - name: Run smoke tests
        run: |
          npm run test:smoke -- --env staging
      
      - name: Notify Slack
        if: always()
        uses: 8398a7/action-slack@v3
        with:
          status: ${{ job.status }}
          text: 'Staging deployment ${{ job.status }}'
          webhook_url: ${{ secrets.SLACK_WEBHOOK }}

  deploy-production:
    name: Deploy to Production
    runs-on: ubuntu-latest
    needs: build
    if: github.ref == 'refs/heads/main'
    environment:
      name: production
      url: https://promoteros.candlefish.ai
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: Update kubeconfig
        run: |
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.EKS_CLUSTER }}
      
      - name: Install Helm
        uses: azure/setup-helm@v3
        with:
          version: '3.13.0'
      
      - name: Deploy canary (10%)
        run: |
          helm upgrade --install promoteros-canary ./helm/charts/promoteros \
            --namespace promoteros-prod \
            --create-namespace \
            --values ./helm/values/prod.yaml \
            --values ./helm/values/canary.yaml \
            --set image.tag=${{ github.sha }} \
            --set canary.enabled=true \
            --set canary.weight=10 \
            --wait \
            --timeout 10m
      
      - name: Monitor canary metrics
        run: |
          echo "Monitoring canary for 10 minutes..."
          npm run monitor:canary -- --duration 600 --threshold 0.01
      
      - name: Full production deployment
        run: |
          helm upgrade --install promoteros ./helm/charts/promoteros \
            --namespace promoteros-prod \
            --values ./helm/values/prod.yaml \
            --set image.tag=${{ github.sha }} \
            --set canary.enabled=false \
            --wait \
            --timeout 15m
      
      - name: Run E2E tests on production
        run: |
          npm run test:e2e -- --env production
      
      - name: Performance test
        run: |
          npm run test:performance -- --env production --assert-slo
      
      - name: Create GitHub release
        if: success()
        uses: actions/create-release@v1
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        with:
          tag_name: v${{ github.sha }}
          release_name: Release ${{ github.sha }}
          body: |
            Production deployment completed successfully
            - Image: ${{ env.ECR_REGISTRY }}/promoteros:${{ github.sha }}
            - Environment: production
            - Cluster: ${{ env.EKS_CLUSTER }}
          draft: false
          prerelease: false
      
      - name: Notify team
        if: always()
        uses: 8398a7/action-slack@v3
        with:
          status: ${{ job.status }}
          text: 'Production deployment ${{ job.status }}'
          webhook_url: ${{ secrets.SLACK_WEBHOOK }}
          fields: repo,message,commit,author,action,eventName,ref,workflow

  rollback:
    name: Rollback Deployment
    runs-on: ubuntu-latest
    if: failure() && (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/develop')
    needs: [deploy-staging, deploy-production]
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: Update kubeconfig
        run: |
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.EKS_CLUSTER }}
      
      - name: Rollback deployment
        run: |
          NAMESPACE="promoteros-staging"
          if [[ "${{ github.ref }}" == "refs/heads/main" ]]; then
            NAMESPACE="promoteros-prod"
          fi
          
          helm rollback promoteros -n $NAMESPACE
          kubectl rollout status deployment -n $NAMESPACE
      
      - name: Notify team of rollback
        uses: 8398a7/action-slack@v3
        with:
          status: 'warning'
          text: 'Deployment rolled back due to failure'
          webhook_url: ${{ secrets.SLACK_WEBHOOK }}